{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYkYm4u35yv-"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install langchain-huggingface\n",
        "\n",

        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install langchain\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import os\n",
        "\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [

        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"HF_tkn\")"
      ],
      "metadata": {
        "id": "j0EkplHf7NI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_tkn\"] = sec_key"
      ],
      "metadata": {
        "id": "bhkG9h2z-Cju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",

        "model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
        "device = 0\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(f'cuda:{device}')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",

        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",

        "output = generator(\"Once upon a time\", do_sample=True, min_length=50, max_length=100, temperature=0.7)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rp8i6W-F1w0",
        "outputId": "d98905e3-9cb1-43c5-8e8d-274e0a09157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, my aunt was a very good cook, and a very good woman. I can still remember when she cooked for me as a child. It was a big family affair, with everyone sitting around the table and everyone eating.\n",
            "\n",
            "I was eight or nine years old, sitting on my mother’s lap while she was making us a delicious meal. As I sat there, I gazed at my aunt, who had the most beautiful smile on her face. It was so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = generator(\"Here on Earth\", do_sample=True, min_length=50, max_length=256, temperature=0.7, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOGvzy95JJJr",
        "outputId": "184ed74c-03cd-4120-9e9b-7225c6e0b551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here on Earth, we have the luxury of having the free reign of our imaginations, and we’ve been able to explore our deepest fears, hopes, and desires for millennia and continue to do so.\n",
            "\n",
            "By comparison, in the alien world of The Fifth Element, the “free reign” of imagination, however, is completely out of the question. While the “alien world” of the movie (or at least its world of dreams) is a fantasy, it’s a fantasy that’s not based in reality.\n",
            "\n",
            "In fact, it’s a fantasy that’s been around for thousands of years. In fact, the idea of “dreams” or “fantasies” is a relatively recent one, developed in the early 1600s by the philosopher and theologian, René Descartes, who wrote in his 1641 book, Discourse on Method that “dreams” are “a confused idea of a thing that is not in fact there.”\n",
            "\n",
            "However, in order to truly grasp the concept of dreams, you need to understand the “real world” of our actual experience.\n",
            "\n",
            "To do that,\n"
          ]
        }
      ]
    }
  ]
}
